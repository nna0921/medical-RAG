{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":127612,"sourceType":"datasetVersion","datasetId":64826}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q langchain langchain-google-genai langchain-community faiss-cpu sentence-transformers ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T01:14:56.503714Z","iopub.execute_input":"2025-11-28T01:14:56.504284Z","iopub.status.idle":"2025-11-28T01:16:17.939270Z","shell.execute_reply.started":"2025-11-28T01:14:56.504259Z","shell.execute_reply":"2025-11-28T01:16:17.938546Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m450.8/450.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ngoogle-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\nfrom langchain_community.document_loaders import DataFrameLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.auto import tqdm  # <--- Library for Progress Bars\n\n# 1. Setup Secrets\nuser_secrets = UserSecretsClient()\nos.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n\n# 2. Load Data\nDATA_PATH = \"/kaggle/input/medicaltranscriptions/mtsamples.csv\"\n\ndef process_data():\n    print(\"Loading Data...\")\n    df = pd.read_csv(DATA_PATH)\n    \n    # Simple cleaning\n    df = df[['transcription', 'medical_specialty', 'description']]\n    df = df.dropna(subset=['transcription'])\n    \n    # --- SAMPLE SIZE ---\n    # Keep at 200 for testing, increase to 800-1000 for final demo\n    df = df.head(800) \n    \n    loader = DataFrameLoader(df, page_content_column=\"transcription\")\n    documents = loader.load()\n    \n    # Split\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n    chunks = text_splitter.split_documents(documents)\n    \n    print(f\"Total chunks to embed: {len(chunks)}\")\n\n    # --- RATE LIMITING SETUP ---\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n    \n    batch_size = 20 \n    sleep_time = 2 \n\n    print(\"Creating Vector Store...\")\n\n    # 1. Initialize Vector Store with FIRST batch\n    # We wrap the operation in a try/except just in case the first one fails\n    try:\n        first_batch = chunks[:batch_size]\n        vector_store = FAISS.from_documents(first_batch, embeddings)\n        time.sleep(sleep_time) \n    except Exception as e:\n        print(f\"Error on first batch: {e}\")\n        return\n\n    # 2. Loop through remaining batches with Progress Bar\n    # We create a range starting from the second batch\n    batch_indices = range(batch_size, len(chunks), batch_size)\n    \n    # tqdm(batch_indices) creates the visual bar\n    for i in tqdm(batch_indices, desc=\"Embedding Batches\", unit=\"batch\"):\n        batch = chunks[i : i + batch_size]\n        \n        # Add new batch to existing index\n        vector_store.add_documents(batch)\n        \n        # Respect rate limit\n        time.sleep(sleep_time)\n\n    print(\"\\nSuccess! All batches processed.\")\n    \n    # Save to the writable directory\n    save_path = \"/kaggle/working/faiss_index\"\n    vector_store.save_local(save_path)\n    print(f\"Done! Index saved to {save_path}\")\n\nif __name__ == \"__main__\":\n    process_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T01:16:17.940811Z","iopub.execute_input":"2025-11-28T01:16:17.941093Z"}},"outputs":[{"name":"stdout","text":"Loading Data...\nTotal chunks to embed: 3873\nCreating Vector Store...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Embedding Batches:   0%|          | 0/193 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317f0a655e2c440d9eb061fdcb00e144"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport os\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom dotenv import load_dotenv\n\n# 1. Load Environment Variables\nload_dotenv()\n\n# 2. Setup RAG Pipeline\nprint(\"Loading RAG Pipeline...\")\n\ntry:\n    # A. Setup Embeddings\n    # \"models/text-embedding-004\" is the stable embedding model\n    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n    \n    # B. Load the FAISS Index\n    # Check if the folder exists\n    if not os.path.exists(\"faiss_index\"):\n        raise FileNotFoundError(\"The 'faiss_index' folder was not found. Please create it and move your index files inside.\")\n        \n    vector_store = FAISS.load_local(\n        \"faiss_index\", \n        embeddings, \n        allow_dangerous_deserialization=True\n    )\n    \n    # C. Setup Retriever\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n    \n    # D. Setup LLM\n    # Updated to 'gemini-2.5-flash' to prevent 404 errors on older model names\n    llm = ChatGoogleGenerativeAI(\n        model=\"gemini-2.5-flash\", \n        temperature=0.3\n    )\n    \n    # E. Prompt\n    template = \"\"\"\n    You are a professional medical assistant. Use the provided context to answer the user's question.\n    If the answer is not in the context, state \"I do not have enough information in my clinical database.\"\n    \n    Context: {context}\n    \n    Question: {question}\n    \n    Answer:\n    \"\"\"\n    prompt = PromptTemplate.from_template(template)\n    \n    # F. Chain\n    rag_chain = (\n        {\"context\": retriever, \"question\": RunnablePassthrough()}\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n    print(\"Pipeline Loaded Successfully!\")\n\nexcept Exception as e:\n    print(f\"CRITICAL ERROR: {e}\")\n    rag_chain = None\n\n# 3. Define the Chat Function\ndef ask_doctor(message, history):\n    # 'history' is now a list of dictionaries [{'role': 'user', 'content': '...'}, ...]\n    # We don't strictly need it for this simple RAG, but it's required by the function signature.\n    \n    if rag_chain is None:\n        return \"Error: The medical database failed to load. Please check your API key and index files.\"\n    \n    if not message:\n        return \"\"\n        \n    try:\n        response = rag_chain.invoke(message)\n        return response\n    except Exception as e:\n        return f\"An error occurred: {str(e)}\"\n\n# 4. Build the Gradio Interface\ndemo = gr.ChatInterface(\n    fn=ask_doctor,\n    type=\"messages\", # <--- Silences the DeprecationWarning\n    title=\"ğŸ©º Medical RAG Assistant\",\n    description=\"Ask questions about medical transcriptions. Answers are based strictly on the provided clinical dataset.\",\n    examples=[\n        \"What are the symptoms of allergic rhinitis?\",\n        \"Describe the surgery for carpal tunnel.\",\n        \"What is the treatment for chronic back pain?\"\n    ],\n    theme=\"soft\"\n)\n\n# 5. Launch\nif __name__ == \"__main__\":\n    demo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain_google_genai import GoogleGenerativeAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\n# Setup\nuser_secrets = UserSecretsClient()\nos.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n\n# Load Index\ntry:\n    vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n    \n    # TEST QUERY\n    query = \"symptoms of allergic rhinitis\"\n    print(f\"Querying: '{query}'\\n\")\n    \n    docs = retriever.invoke(query)\n    \n    if not docs:\n        print(\"No documents found! Your index might be empty.\")\n    else:\n        print(f\"Found {len(docs)} documents.\\n\")\n        for i, doc in enumerate(docs):\n            print(f\"--- Document {i+1} ---\")\n            print(doc.page_content[:300] + \"...\") # Print first 300 chars\n            print(f\"[Source: {doc.metadata.get('medical_specialty', 'Unknown')}]\\n\")\n            \nexcept Exception as e:\n    print(f\"Error: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport time\nimport os\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom kaggle_secrets import UserSecretsClient\nfrom tqdm.auto import tqdm \n\n# --- SETUP ---\nprint(\"Initializing Evaluation Pipeline...\")\n\n# 1. Secrets\ntry:\n    user_secrets = UserSecretsClient()\n    os.environ[\"GOOGLE_API_KEY\"] = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nexcept:\n    pass \n\n# 2. Load Components\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\nvector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\nretriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n\n# 3. Chain\ntemplate = \"\"\"\nYou are a medical assistant analyzing clinical notes. \nAnswer the question based on the patient records provided below. \nIf the answer is not in the context, state \"I do not have enough information.\"\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:\n\"\"\"\nprompt = PromptTemplate.from_template(template)\nrag_chain = (\n    {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\n# --- 30 QUESTIONS LIST ---\nquestions = [\n    \"What are the symptoms of allergic rhinitis?\",\n    \"Describe the procedure for a colonoscopy.\",\n    \"What is the treatment for chronic back pain?\",\n    \"List the side effects of Lisinopril.\",\n    \"What are the common symptoms of bronchitis?\",\n    \"How is appendicitis diagnosed?\",\n    \"What post-operative care is needed for tonsillectomy?\",\n    \"Describe the symptoms of otitis media.\",\n    \"What is the treatment for carpal tunnel syndrome?\",\n    \"What are the signs of a rotator cuff tear?\",\n    \"Explain the procedure for a laparoscopic cholecystectomy.\",\n    \"What are the symptoms of sleep apnea?\",\n    \"How is a hernia repaired?\",\n    \"What is the management for hypertension?\",\n    \"Describe the symptoms of GERD.\",\n    \"What are the complications of diabetes mellitus?\",\n    \"How is a lumbar puncture performed?\",\n    \"What is the treatment for migraine headaches?\",\n    \"Describe the signs of a hip fracture.\",\n    \"What are the symptoms of pneumonia?\",\n    \"How is a cataract removed?\",\n    \"What is the treatment for anxiety disorder?\",\n    \"Describe the symptoms of a urinary tract infection.\",\n    \"What is the procedure for a total knee replacement?\",\n    \"How is a skin abscess treated?\",\n    \"What are the symptoms of vertigo?\",\n    \"Describe the management of chronic obstructive pulmonary disease (COPD).\",\n    \"What is the treatment for acne vulgaris?\",\n    \"How is a deviated septum corrected?\",\n    \"What are the symptoms of osteoarthritis?\"\n]\n\n# --- RUN EVALUATION ---\nprint(f\"Starting evaluation of {len(questions)} queries...\")\nprint(\"Note: This will take ~5 minutes to avoid rate limits.\")\nresults = []\n\nfor q in tqdm(questions, desc=\"Evaluating\"):\n    try:\n        # Get answer\n        start_time = time.time()\n        answer = rag_chain.invoke(q)\n        duration = time.time() - start_time\n        \n        # Check if it found info or not\n        status = \"Success\"\n        if \"not have enough information\" in answer:\n            status = \"No Context Found\"\n            \n        results.append({\n            \"Question\": q,\n            \"Answer\": answer,\n            \"Response Time (s)\": round(duration, 2),\n            \"Status\": status\n        })\n        \n    except Exception as e:\n        results.append({\n            \"Question\": q,\n            \"Answer\": f\"ERROR: {str(e)}\",\n            \"Response Time (s)\": 0,\n            \"Status\": \"Error\"\n        })\n        # If we hit an error, wait even longer to let the API cool down\n        time.sleep(20)\n    \n    # --- CRITICAL CHANGE: INCREASE SLEEP TO 10 SECONDS ---\n    # The API limit is 10 requests per minute (1 request every 6 seconds).\n    # We sleep 10s to be safe.\n    time.sleep(10)\n\n# --- SAVE REPORT ---\ndf = pd.DataFrame(results)\nfilename = \"evaluation_report.csv\"\ndf.to_csv(filename, index=False)\n\nprint(\"\\nEvaluation Complete!\")\nprint(f\"Success Rate: {len(df[df['Status']=='Success'])}/{len(questions)}\")\nprint(f\"Report saved to: {filename}\")\nprint(\"(Check the 'Output' sidebar to download it)\")","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null}]}